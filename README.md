# Dynamic-CNN-Sentence-Classification-TF
一.介绍

算法实现：https://github.com/lc222/Dynamic-CNN-Sentence-Classification-TF

这篇论文介绍了一种DCNN对句子语义建模，时间有点久远了就是经典回来再学习一下，代码实现一下。模型采用动态K-max pooling取出得分top k的特征值，这里区别一下与max pooling 的区别，能处理不同的句子，不依赖解析树，而word2vec是依赖与huffman树的，词向量

方法进行文本分类是依赖解析树的，所有词为叶子节点，从根到叶子节点的路径编码为词向量的表示形式。

二.模型的特点

1 保留了句子中词序信息和词语之间的相对位置；

2 宽卷积的结果是传统卷积的一个扩展，某种意义上，也是n-gram的一个扩展，更加考虑句子边缘信息；

3 模型不需要任何的先验知识，例如句法依存树等，并且模型考虑了句子中相隔较远的词语之间的语义信息。



上图就是一个动态CNN处理输入句子的过程，卷积filters的宽度是2和3，通过动态pooling，可以在高层获取句子中相离较远词语间的联系。

三.下面给出DCNN模型及介绍：



模型具体流程操作可总结为5部分：

1：宽卷卷部分：宽卷积的输出使feature map 的宽度更宽，类似n-gram。

2：k-max pooling：

给定一个K值和一个序列P，选择序列P中得分最大的前k个特征值组成的子序列，并且保留原来序列的次序。

3:动态K-max pooling:

K和k分别是输入句子长度和网络深度两个参数的函数



L代表是网络模型卷积层的总层数，l代表的具体在某一层卷积上的层数，s代表的是句子的长度，当给你句子长度

s时可以算出每个卷积层后面的值，例如：网络卷积层有5个，ktop=5，s=15，则k1=15*（4/5）=12,k2=9,k5=5.

动态K-max pooling的意义在于从不同长度的句子中提取相应数量的语义特征信息，以保证后续的卷积层的统一，

从s中提取k1个特征，再提取k2个特征，...一次类推。

4：多特征图（multi-feature map）

提取多个feature map以保证提取特征的多样性。

5：折叠操作

Folding操作则是考虑相邻的两行之间的某种联系，方式也很简单，就是将两行的vector相加；该操作没有增加参数数量，

但是提前（在最后的全连接层之前）考虑了特征矩阵中行与行之间的某种关联，考虑两个句子之间的关联性。

四.模型训练与实验结果

输出层是一个类别概率分布（即softmax），与倒数第二层全连接；

代价函数为交叉熵，训练目标是最小化代价函数；L2正则化；

优化方法：mini-batch + gradient-based (使用Adagrad update rule, Duchi et al., 2011)

在三个数据集上进行了实验，分别是(1)电影评论数据集上的情感识别，(2)TREC问题分类，以及(3)Twitter数据集上的情感识别。





DCNN模型与传统模型效果稍好，DCNN模型能全面捕获词的信息，更好的表示整个句子。
--------------------- 
作者：gentelyang 
来源：CSDN 
原文：https://blog.csdn.net/gentelyang/article/details/80811047 
版权声明：本文为博主原创文章，转载请附上博文链接！
